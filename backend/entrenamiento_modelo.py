import pandas as pd
import pickle
from sqlalchemy import create_engine
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors

"""
-----------------------------------------------------------------------------
M√ìDULO DE ENTRENAMIENTO DEL MODELO DE RECOMENDACI√ìN
TRABAJO TERMINAL
-----------------------------------------------------------------------------
Descripci√≥n:
    Este script orquesta el proceso ETL (Extracci√≥n, Transformaci√≥n y Carga)
    para el motor de recomendaci√≥n. Se encarga de conectar a la base de datos
    PostgreSQL, vectorizar el contenido textual mediante modelos BERT y
    entrenar el algoritmo de vecinos m√°s cercanos (KNN).

Autores: [Cuellar Reyes Ethan Mois√©s, Gonz√°lez Rojo Scarlett Michelle y Luna Zamora Juan Antonio]
-----------------------------------------------------------------------------
"""

# ------------------------------------------------------------
# CONFIGURACI√ìN DE LA INFRAESTRUCTURA DE DATOS
# ------------------------------------------------------------
# Definimos las credenciales de acceso a nuestra base de datos local.
# Nota: Para el despliegue final, estas credenciales se migrar√°n a 
# variables de entorno por seguridad.
DB_USER = "postgres"
DB_PASS = "ns+E{XJ_ohhj9EQ)" 
DB_HOST = "34.123.117.28"
DB_PORT = "5432"
DB_NAME = "mexcine_db"

# Especificamos la tabla fuente normalizada que contiene el cat√°logo
TABLE_NAME = "peliculas"
# ------------------------------------------------------------


def obtener_datos():
    """
    Establece la conexi√≥n con la capa de persistencia (PostgreSQL) y recupera
    el dataset necesario para el entrenamiento.
    
    Returns:
        pd.DataFrame: DataFrame con las columnas 'anio', 'titulo', 'genero' y 'sinopsis'.
    
    Raises:
        ValueError: Si la consulta no retorna registros, detenemos el flujo para evitar errores en el modelo.
    """
    # Construimos la cadena de conexi√≥n utilizando el formato est√°ndar de SQLAlchemy
    connection_url = f"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    engine = create_engine(connection_url)

    # Seleccionamos √∫nicamente las caracter√≠sticas (features) relevantes para la similitud sem√°ntica
    query = f"""
        SELECT anio, titulo, genero, sinopsis
        FROM {TABLE_NAME};
    """

    print("--- [FASE 1] Iniciando extracci√≥n de datos ---")
    print(f"Conectando a base de datos: {DB_NAME} en {DB_HOST}...")
    
    df = pd.read_sql(query, engine)

    # Validaci√≥n de integridad de los datos
    if df.empty:
        raise ValueError("Error cr√≠tico: La tabla est√° vac√≠a. No es posible entrenar el modelo.")

    print(f"√âxito: Se han cargado {len(df)} registros en memoria.")
    return df


def generar_embeddings(df):
    """
    Transforma el lenguaje natural (sinopsis y g√©nero) en representaciones vectoriales densas.
    Utilizamos Sentence-BERT para capturar el contexto sem√°ntico de cada pel√≠cula.
    
    Args:
        df (pd.DataFrame): Dataset crudo.
        
    Returns:
        numpy.ndarray: Matriz de embeddings listos para el c√°lculo de distancias.
    """
    print("\n--- [FASE 2] Vectorizaci√≥n de texto (NLP) ---")
    
    # Seleccionamos 'all-MiniLM-L6-v2' por ser el compromiso √≥ptimo entre 
    # velocidad de inferencia y precisi√≥n sem√°ntica para nuestra infraestructura.
    print("Cargando modelo pre-entrenado: all-MiniLM-L6-v2...")
    model = SentenceTransformer("all-MiniLM-L6-v2")

    # Ingenier√≠a de caracter√≠sticas: Concatenamos g√©nero y sinopsis para enriquecer 
    # el contexto del vector resultante. Manejamos valores nulos para evitar fallos.
    textos = (df["genero"].fillna("") + " " + df["sinopsis"].fillna("")).tolist()

    print(f"Generando embeddings para {len(textos)} elementos...")
    embeddings = model.encode(textos, show_progress_bar=True)

    return embeddings


def entrenar_knn(embeddings, k=5):
    """
    Entrena el modelo de vecinos m√°s cercanos (NearestNeighbors).
    
    Optamos por KNN debido a su eficiencia en sistemas de recomendaci√≥n basados en contenido,
    donde la proximidad en el espacio vectorial indica similitud tem√°tica.
    
    Args:
        embeddings (numpy.ndarray): Matriz de vectores.
        k (int): N√∫mero de vecinos a considerar (default: 5).
        
    Returns:
        sklearn.neighbors.NearestNeighbors: Modelo ajustado.
    """
    print("\n--- [FASE 3] Entrenamiento del modelo (KNN) ---")
    
    # Utilizamos la m√©trica 'cosine' ya que es independiente de la magnitud del vector,
    # midiendo puramente la orientaci√≥n (similitud sem√°ntica) en espacios de alta dimensi√≥n.
    print(f"Configurando KNN con m√©trica 'cosine' y k={k}...")
    knn = NearestNeighbors(n_neighbors=k, metric="cosine")
    
    knn.fit(embeddings)
    print("Modelo ajustado correctamente.")
    return knn


def guardar_archivos(knn, embeddings, df):
    """
    Serializa los objetos cr√≠ticos del sistema para su uso en el backend (API).
    Almacenamos el modelo, los vectores y los metadatos para evitar re-entrenamientos 
    en cada petici√≥n del usuario.
    """
    print("\n--- [FASE 4] Persistencia de artefactos ---")

    # Guardamos el modelo KNN
    with open("backend/modelo_knn.pkl", "wb") as f:
        pickle.dump(knn, f)

    # Guardamos la matriz de embeddings (necesaria para futuras inferencias)
    with open("backend/embeddings.pkl", "wb") as f:
        pickle.dump(embeddings, f)

    # Guardamos el cat√°logo como diccionario para una b√∫squeda r√°pida (O(1)) por √≠ndice
    with open("backend/peliculas_info.pkl", "wb") as f:
        pickle.dump(df.to_dict(), f)

    print("‚úÖ Serializaci√≥n completada. Archivos generados en /backend:")
    print("   -> modelo_knn.pkl (L√≥gica de recomendaci√≥n)")
    print("   -> embeddings.pkl (Espacio vectorial)")
    print("   -> peliculas_info.pkl (Metadatos)")


# Punto de entrada principal
if __name__ == "__main__":
    try:
        # Orquestaci√≥n secuencial del pipeline
        df_peliculas = obtener_datos()
        matriz_embeddings = generar_embeddings(df_peliculas)
        modelo_entrenado = entrenar_knn(matriz_embeddings)
        guardar_archivos(modelo_entrenado, matriz_embeddings, df_peliculas)

        print("\nüéâ Proceso finalizado: El sistema de recomendaci√≥n est√° listo para despliegue.\n")
        
    except Exception as e:
        print("\n‚ö† Excepci√≥n cr√≠tica durante la ejecuci√≥n del pipeline:")
        print(f"Detalle del error: {str(e)}")